{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e6bf1d",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming is the process of reducing words to their root or stem form by removing suffixes, allowing different variations of a word to be treated as the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5efe342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "192d82cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()\n",
    "words = ['run','runner','ran','runs','easily','fairly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "338f35fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run----->run\n",
      "runner----->runner\n",
      "ran----->ran\n",
      "runs----->run\n",
      "easily----->easili\n",
      "fairly----->fairli\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + '----->' + p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e53a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78369d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7097034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run ----->run\n",
      "runner ----->runner\n",
      "ran ----->ran\n",
      "runs ----->run\n",
      "easily ----->easili\n",
      "fairly ----->fair\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + ' ----->' + s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eddf0f",
   "metadata": {},
   "source": [
    "### Lemmas\n",
    "\n",
    "\n",
    "A lemma is the base or canonical form of a word, and lemmatization is the process of reducing words to their base forms. It helps normalize text data and extract core meanings, improving accuracy in various natural language processing tasks.\n",
    "\n",
    "Lemmas tend to be more accurate, but more computationally expensive as they taken context around the word into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d46cd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t 4690420944186131903 \t I\n",
      "am \t AUX \t 10382539506755952630 \t be\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "runner \t NOUN \t 12640964157389618806 \t runner\n",
      "in \t ADP \t 3002984154512732771 \t in\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "race \t NOUN \t 8048469955494714898 \t race\n",
      "because \t SCONJ \t 16950148841647037698 \t because\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "love \t VERB \t 3702023516439754181 \t love\n",
      "to \t PART \t 3791531372978436496 \t to\n",
      "run \t VERB \t 12767647472892411841 \t run\n",
      "since \t SCONJ \t 10066841407251338481 \t since\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "ran \t VERB \t 12767647472892411841 \t run\n",
      "today \t NOUN \t 11042482332948150395 \t today\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc1 = nlp(u\"I am a runner in a race because I love to run since I ran today\")\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.text,'\\t',token.pos_,'\\t', token.lemma, '\\t', token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "793cb3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   4690420944186131903    I\n",
      "saw          VERB   11925638236994514241   see\n",
      "ten          NUM    7970704286052693043    ten\n",
      "mice         NOUN   1384165645700560590    mouse\n",
      "today        NOUN   11042482332948150395   today\n"
     ]
    }
   ],
   "source": [
    "# Alternative way of showign lemmas\n",
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')\n",
    "        \n",
    "doc2 = nlp(u\"I saw ten mice today\")\n",
    "show_lemmas(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6af6d3",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "How to access spacy's default stop words (326) and add/remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1470eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e549f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43d32572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mystery'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How to tell if 'is' a stopword\n",
    "display('is',nlp.vocab['is'].is_stop)\n",
    "\n",
    "display('mystery',nlp.vocab['mystery'].is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52f2e25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to add stopwords (+validation)\n",
    "nlp.Defaults.stop_words.add('btw')\n",
    "nlp.vocab['btw'].is_stop = True\n",
    "\n",
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54318203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to remove stopwords\n",
    "nlp.vocab['beyond'].is_stop = False\n",
    "nlp.vocab['beyond'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23479291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
